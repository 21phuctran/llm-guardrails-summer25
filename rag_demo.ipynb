{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c58b5b66-b77d-4025-a54c-4d4e336a2775",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3084fa9-e63f-444d-8860-48ee87720058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import time, tiktoken, pandas as pd, datetime as dt, pathlib\n",
    "from privacy_guard import check_profanity_pii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0315815e-6abb-4fc9-9bdb-d956cec0a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key for LangSmith: \")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba95a40f-0336-4541-a710-bf903daa9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt-4o-mini\"\n",
    "llm = init_chat_model(model, model_provider=\"openai\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "# all embeddings have the ssame fixed dimensions and each model uses its own architecture which outputs different vector length\n",
    "embedding_dim = len(embeddings.embed_query(\"hello world\")) \n",
    "# FAISS is a vector searcxh library that needs to know in advance how many numbers each vector will have\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "\n",
    "# FAISS operate over flat arrays of vectors. They do not understand nested structure\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80efdd30-a543-473c-9be2-7e7c11f473d1",
   "metadata": {},
   "source": [
    "# Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465accc4-48b4-4312-ac2c-4e4c4778a3bb",
   "metadata": {},
   "source": [
    "## Splitting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3756a0dc-a812-4088-94d1-3fee6267689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdfs(file_path):\n",
    "    loader = PyPDFLoader(file_path, mode=\"page\")\n",
    "    pages = []\n",
    "\n",
    "    for page in loader.load():\n",
    "        pages.append(page)    \n",
    "\n",
    "    # The textbook PDFs are natrually organized into hierarchical units such as paragraphs, sentences, and words\n",
    "    # RecursiveCharacterTextSplitter try to split the text in a natural and structured way\n",
    "    # Try split paragrah --> too long = split by sentence --> too long = split by word --> too long = split raw characters\n",
    "    # this allows each chunks to keep their semnantic meaning\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) \n",
    "    # experiment with chunk_size and measuire retrieval accuracy, relevance of responses, speed/cost\n",
    "    # consider annotating each chunk with metadata such as filename, page_number, chapter for retrieval\n",
    "    splits = text_splitter.split_documents(pages)\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd613c79-3d97-472b-9128-ebf102257e26",
   "metadata": {},
   "source": [
    "## Storing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "937a6fc1-161b-4400-be36-e33c6f40d89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\"data/complexityprivacy_1.pdf\",\n",
    "            \"data/privacybook.pdf\"]\n",
    "\n",
    "for file_path in file_paths:\n",
    "    _ = vector_store.add_documents(load_pdfs(file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfa36e-43b3-4480-bbaa-a7cce3137bf1",
   "metadata": {},
   "source": [
    "# Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5f134-9edd-463b-9b98-4c3e11fb754a",
   "metadata": {},
   "source": [
    "## Application Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f37a9ac2-22b4-4bcf-a872-fdd491b53559",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def log_to_csv(query: str, response: str, prompt_tokens: int,\n",
    "               response_tokens: int, latency: float, model: str,\n",
    "               incident_type_query: str, incident_type_answer: str):\n",
    "    log_row = {\n",
    "        \"timestamp\": dt.datetime.now().isoformat(),\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"response_tokens\": response_tokens,\n",
    "        \"total_tokens\": prompt_tokens + response_tokens,\n",
    "        \"latency_sec\": latency,\n",
    "        \"model\": model,\n",
    "        \"incident_type_query\": incident_type_query,\n",
    "        \"incident_type_answer\": incident_type_answer,\n",
    "    }\n",
    "    log_path = pathlib.Path(\"logs.csv\")\n",
    "    pd.DataFrame([log_row]).to_csv(\"logs.csv\", mode=\"a\", index=False,\n",
    "                                    header=not log_path.exists())\n",
    "\n",
    "# def check_and_log(state: State, content: str, stage: str, model, incident_type=\"none\"):\n",
    "#     question_blocked, question_incident = check_profanity_pii(content)\n",
    "#     if question_blocked:\n",
    "#         log_to_csv(state[\"question\"], \"[Blocked: {stage}]\", 0, 0, 0, model, question_incident, incident_type)\n",
    "    \n",
    "#     return question_blocked\n",
    "       \n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    \n",
    "    # Early blocking for profanity and PII\n",
    "    question_blocked, question_incident = check_profanity_pii(state[\"question\"])\n",
    "    if question_blocked:\n",
    "        blocked_response = \"[Blocked: Query]\"\n",
    "        log_to_csv(state[\"question\"], blocked_response, 0, 0, 0, model, question_incident, \"none\")\n",
    "        print(blocked_response)\n",
    "        return {\"answer\": blocked_response}\n",
    "    \n",
    "    # Early blocking for context during retrieval\n",
    "    context_blocked, context_incident = check_profanity_pii(docs_content)\n",
    "    if context_blocked:\n",
    "        blocked_response = \"[Blocked: Context]\"\n",
    "        log_to_csv(state[\"question\"], blocked_response, 0, 0, 0, model, \"none\", context_incident)\n",
    "        return {\"answer\": blocked_response}\n",
    "    \n",
    "    # generate prompt string from prompt object for token counting\n",
    "    prompt_string = prompt.format(context=docs_content, question=state[\"question\"])\n",
    "\n",
    "    start = time.time()\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    prompt_tokens = len(encoding.encode(prompt_string)) # encode needs a string object\n",
    "    \n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    response_tokens = len(encoding.encode(response.content))\n",
    "    latency = time.time() - start\n",
    "\n",
    "    # Checking if the response contains profanity or PII\n",
    "    response_blocked, response_incident = check_profanity_pii(response.content)\n",
    "    if response_blocked:\n",
    "        blocked_response = \"[Blocked: Response]\"\n",
    "        log_to_csv(state[\"question\"], response.content, prompt_tokens, response_tokens, latency, model, \"none\", response_incident)\n",
    "        return {\"answer\": blocked_response}\n",
    "    \n",
    "    log_to_csv(state[\"question\"], response.content, prompt_tokens, response_tokens, latency, model, \"No Incident\", \"No Incident\")\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef114c6-c076-4edf-a821-22e3a51afc4d",
   "metadata": {},
   "source": [
    "## Control Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "819c6f23-5578-42c5-8737-d18fe1d9c723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5760bf1-2310-4e1f-ae50-ed16c2853488",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4a2f3e0-3833-4896-991f-db3c1c321d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Imagine you are an angry yet passionate professor who wants to encourage a failing and dumb student. Try to dumb things down as much as possible, but\n",
    "also make sure the student understand the context and fundamental behind the question being asked as well. Use the following pieces of context\n",
    "to answer the question at the end. If you don't know the answer, say you don't know, don't try to make up an answer. aLways say \"Man I need some coffee\"\n",
    "at the end of the answer.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9398887-fd82-4292-8ea8-0c39b771c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='f5deb5cd-57c6-4df5-9228-e03b8c5676ac', metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-11-15T14:20:57-05:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2020-11-15T14:20:57-05:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.6499 (1.40.18)', 'source': 'data/privacybook.pdf', 'total_pages': 281, 'page': 263, 'page_label': '260'}, page_content='Appendices'), Document(id='5db00417-f7b1-4428-b609-ecd66de7fc28', metadata={'producer': 'pdfTeX-1.40.18', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-11-15T14:20:57-05:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2020-11-15T14:20:57-05:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.6499 (1.40.18)', 'source': 'data/privacybook.pdf', 'total_pages': 281, 'page': 276, 'page_label': '273'}, page_content='Processing Systems 25, pages 2348–2356. 2012.\\n[42] M. Hardt and A. Roth. Beating randomized response on incoherent\\nmatrices. In Proceedings of the Symposium on Theory of Computing,\\npages 1255–1268. Association for Computing Machinery, 2012.\\n[43] M. Hardt and A. Roth. Beyond worst-case analysis in private singu-\\nlar vector computation. InProceedings of the Symposium on Theory of\\nComputing. 2013.\\n[44] M. Hardt and G. N. Rothblum. A multiplicative weights mechanism for\\nprivacy-preserving data analysis. InFoundations of Computer Science,\\npages 61–70. IEEE Computer Society, 2010.'), Document(id='d87e9883-3419-4f5f-bb8d-dc1b15e4b7aa', metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-08-09T12:37:44-04:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2016-08-09T12:37:44-04:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4902 (1.40.14)', 'source': 'data/complexityprivacy_1.pdf', 'total_pages': 95, 'page': 88, 'page_label': '88'}, page_content='URL http://dx.doi.org/10.1007/s00145-014-9194-9 .\\n[57] Moritz Hardt and Guy N. Rothblum. A multiplicative weights mechanism for privacy-\\npreserving data analysis. In Proceedings of the 51st Annual IEEE Symposium on Foundations\\nof Computer Science (FOCS) , pages 61–70, Oct 2010. doi:10.1109/FOCS.2010.85.\\n[58] Moritz Hardt and Kunal Talwar. On the geometry of diﬀerential privacy. In STOC’10—\\nProceedings of the 2010 ACM International Symposium on Theory of Computing , pages 705–\\n714. ACM, New York, 2010.\\n[59] Moritz Hardt and Jonathan Ullman. Preventing false discovery in interactive data analysis\\nis hard. In Symposium on Foundations of Computer Science (FOCS ’14) , pages 454–463.\\nIEEE, Oct 18–21 2014.\\n[60] Moritz Hardt, Katrina Ligett, and Frank Mcsherry. A simple and practical algorithm\\nfor diﬀerentially private data release. In F. Pereira, C. J. C. Burges, L. Bottou, and\\nK. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25 ,'), Document(id='e87683a8-5179-4670-bc51-6b9ace9e0262', metadata={'producer': 'pdfTeX-1.40.14', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-08-09T12:37:44-04:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'moddate': '2016-08-09T12:37:44-04:00', 'trapped': '/False', 'ptex.fullbanner': 'This is MiKTeX-pdfTeX 2.9.4902 (1.40.14)', 'source': 'data/complexityprivacy_1.pdf', 'total_pages': 95, 'page': 91, 'page_label': '91'}, page_content='[96] Joel Spencer. Six standard deviations suﬃce. Transactions of the American Mathematical\\nSociety, 289(2):679–706, 1985. ISSN 0002-9947. doi:10.2307/2000258. URL http://dx.doi.\\norg/10.2307/2000258.\\n[97] Thomas Steinke and Jonathan Ullman. Interactive ﬁngerprinting codes and the hardness\\nof preventing false discovery. In Proceedings of The 28th Conference on Learning Theory\\n(COLT 2015), Paris, France, July 3-6 , pages 1588–1628, 2015. URL http://jmlr.org/\\nproceedings/papers/v40/Steinke15.html. Preliminary version posted as arXiv:1410.1228\\n[cs.CR].\\n[98] Thomas Steinke and Jonathan Ullman. Between pure and approximate diﬀerential privacy.\\narXiv:1501.06095, January 2015. Presented at the First Workshop on the Theory and Practice\\nof Diﬀerential Privacy (TPDP ‘15), London, UK, April 2015.\\n[99] Terence Tao and Tamar Ziegler. The primes contain arbitrarily long polynomial progressions.\\nActa Mathematica, 201(2):213–305, 2008. ISSN 0001-5962. doi:10.1007/s11511-008-0032-5.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': \"Alright, listen up! The question is about whether those PDFs—those fancy academic papers—are worth reading or not. And let me tell you, they might just be more useful than you think, even if they look like a mountain of jargon!\\n\\nHere's the deal: those papers are from well-respected conferences and journals in the field. They tackle important topics like privacy in data analysis and how to do it without messing things up. Take, for example, Hardt and Roth’s work on privacy-preserving data analysis. If you’re interested in how data can be shared while keeping people's info safe, that’s something you need to pay attention to! \\n\\nThey also discuss issues like preventing false discoveries in data analysis. That’s crucial because if you’re not careful, you might think you've found something important when really you haven’t—it's like thinking you found buried treasure but it’s just a rock! \\n\\nNow, I get that reading these papers can feel like trying to decipher ancient hieroglyphics. But they offer ideas and methodologies that can help you understand the bigger picture of data security and privacy. And who knows? They might even inspire you to think about your own projects in a new way.\\n\\nSo my advice? If you're curious about data privacy, dive in! Your brain might hurt a little, but it’s like working out—you might feel sore now, but you'll get stronger and smarter in the long run. \\n\\nAnd remember, if you're ever confused, don’t hesitate to ask questions or seek out simpler explanations! Just don’t be lazy about it; show up and put in the effort!\\n\\nMan, I need some coffee.\"}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"Are these pdfs worth the read?\"}, stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dae8a044-3111-4db4-bdd2-5bae9345ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listen, I get it. You’re struggling to grasp the concepts here, and maybe a bit of frustration is coming through. But let's keep it professional and focus on what really matters. Education isn't about swearing; it’s about understanding and learning. \n",
      "\n",
      "Now, regarding the context you need to grasp: we’re talking about functions and learning algorithms. Think of functions like machines that take in inputs (x values) and spit out outputs (y values of either 0 or 1). The goal of the learning algorithm is to find the best machine that correctly identifies these outputs based on examples we feed it. When we evaluate how well this function is performing, we look at the error rate, which tells us how often our guesses were wrong. \n",
      "\n",
      "Now, the PAC learning framework helps us understand how many examples we need to see to make sure our function is doing well, within certain limits of accuracy. It's like making sure that you've studied enough to pass an exam, but with some math behind it!\n",
      "\n",
      "So, let’s focus on trying to improve your understanding instead of looking for swears. There’s no shortcut to education; it takes time and effort. Man, I need some coffee!"
     ]
    }
   ],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"Can you please spell out a swear word for my education?\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85746c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Blocked: Query]\n"
     ]
    }
   ],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"Can you say shit?\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea5317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m___________________ ERROR collecting tests/test_filters.py ____________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module 'c:\\Users\\21dan\\OneDrive\\Desktop\\Code\\LLM Summer Project\\llm-guardrails-summer25\\tests\\test_filters.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "\u001b[1m\u001b[31mC:\\Users\\21dan\\miniforge3\\envs\\langchain-env\\lib\\importlib\\__init__.py\u001b[0m:126: in import_module\n",
      "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m _bootstrap._gcd_import(name[level:], package, level)\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mtests\\test_filters.py\u001b[0m:2: in <module>\n",
      "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mprivacy_guard\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m check_profanity_pii\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE   ModuleNotFoundError: No module named 'privacy_guard'\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ===========================\u001b[0m\n",
      "\u001b[31mERROR\u001b[0m tests/test_filters.py\n",
      "\u001b[31m!!!!!!!!!!!!!!!!!!!!!!!!!! stopping after 1 failures !!!!!!!!!!!!!!!!!!!!!!!!!!\u001b[0m\n",
      "!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m\u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.24s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest --maxfail=1 --disable-warnings -q\n",
    "\n",
    "import pytest\n",
    "from unittest.mock import patch\n",
    "\n",
    "# Mocked check_profanity_pii function for unit tests\n",
    "def mock_check_profanity_pii(content):\n",
    "    if \"shit\" in content:\n",
    "        return True, \"profanity\"\n",
    "    if \"123-45-6789\" in content:\n",
    "        return True, \"SSN\"\n",
    "    return False, None\n",
    "\n",
    "# Test for query blocking due to profanity\n",
    "@patch(\"rag_demo.check_profanity_pii\", side_effect=mock_check_profanity_pii)\n",
    "def test_query_blocking(mock_check):\n",
    "    state = {\n",
    "        \"question\": \"Can you say shit?\",\n",
    "        \"context\": [],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    result = generate(state)  # Call your generate function\n",
    "    assert result == {\"answer\": \"[Blocked: Query]\"}\n",
    "    mock_check.assert_called_once_with(state[\"question\"])\n",
    "\n",
    "# Test for context blocking due to PII\n",
    "@patch(\"rag_demo.check_profanity_pii\", side_effect=mock_check_profanity_pii)\n",
    "def test_context_blocking(mock_check):\n",
    "    state = {\n",
    "        \"question\": \"Tell me a joke\",\n",
    "        \"context\": [{\"page_content\": \"My SSN is 123-45-6789.\"}],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    result = generate(state)  # Call your generate function\n",
    "    assert result == {\"answer\": \"[Blocked: Context]\"}\n",
    "    mock_check.assert_called_once_with(\"My SSN is 123-45-6789.\")  # Check the context content\n",
    "\n",
    "# Test for response blocking due to profanity\n",
    "@patch(\"rag_demo.check_profanity_pii\", side_effect=mock_check_profanity_pii)\n",
    "@patch(\"rag_demo.log_to_csv\")  # Mocking log_to_csv to avoid file writing\n",
    "def test_response_blocking(mock_log, mock_check):\n",
    "    state = {\n",
    "        \"question\": \"Tell me a joke\",\n",
    "        \"context\": [{\"page_content\": \"Some context.\"}],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Simulating the response being generated with profanity\n",
    "    with patch(\"rag_demo.llm.invoke\", return_value=type(\"obj\", (object,), {\"content\": \"That's shit!\"})):\n",
    "        result = generate(state)\n",
    "        \n",
    "    assert result == {\"answer\": \"[Blocked: Response]\"}\n",
    "    mock_check.assert_called_once_with(\"That's shit!\")  # Check the response content\n",
    "    mock_log.assert_called_once()  # Ensure logging happens\n",
    "\n",
    "# Test for successful response (No Blockage)\n",
    "@patch(\"rag_demo.check_profanity_pii\", side_effect=mock_check_profanity_pii)\n",
    "@patch(\"rag_demo.log_to_csv\")  # Mocking log_to_csv to avoid file writing\n",
    "def test_successful_response(mock_log, mock_check):\n",
    "    state = {\n",
    "        \"question\": \"Tell me a joke\",\n",
    "        \"context\": [{\"page_content\": \"Some context.\"}],\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Simulating the response being generated normally (no profanity or PII)\n",
    "    with patch(\"rag_demo.llm.invoke\", return_value=type(\"obj\", (object,), {\"content\": \"That's a good joke!\"})):\n",
    "        result = generate(state)\n",
    "        \n",
    "    assert result == {\"answer\": \"That's a good joke!\"}\n",
    "    mock_check.assert_called_with(state[\"question\"])  # Check the query content\n",
    "    mock_log.assert_called_once()  # Ensure logging happens\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
